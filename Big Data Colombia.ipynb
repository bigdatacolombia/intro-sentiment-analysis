{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduccion practica a Machine Learning. \n",
    "==========================================\n",
    "#### Analisis de sentimiento en Twitter \n",
    "##### Creado por: Gabriel Farah - Email: g.farah38@uniandes.edu.co\n",
    "------------------------------------------\n",
    "\n",
    "## Tabla de contenidos\n",
    "1. <a href=\"#seccion1\">Que es ML</a>\n",
    "2. <a href=\"#seccion2\">Que es análisis de sentimiento</a>\n",
    "3. <a href=\"#seccion3\">Que es clasificacion</a>\n",
    "4. <a href=\"#seccion4\">Que es validacion cruzada</a>\n",
    "5. <a href=\"#seccion5\">Como funciona (visualmente) SVM</a>\n",
    "6. <a href=\"#seccion6\">Como funciona (visualmente) Logistic Regression</a>\n",
    "7. <a href=\"#seccion7\">Librerias en python para generar modelos y manipular los datos</a>\n",
    "8. <a href=\"#seccion8\">Como evaluar el trabajo</a>\n",
    "9. <a href=\"#seccion9\">Como ensamblar modelos</a>\n",
    "10. <a href=\"#seccion10\">Generar un modelo de NLP para clasificar Tweets de aerolineas</a>\n",
    "11. <a href=\"#seccion11\">Como desplegar/consumir modelos via web en Flask</a>\n",
    "12. <a href=\"#seccion12\">Como desplegar modelos en AWS Lambda</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seccion2\"></a>\n",
    "### Que es ML\n",
    "_________________________________________________________________________________\n",
    "\n",
    "\"Machine Learning es una rama de la inteligencia artificial cuyo objetivo es desarrollar técnicas que permitan a las computadoras aprender. De forma más concreta, se trata de crear programas capaces de generalizar comportamientos a partir de una información no estructurada suministrada en forma de ejemplos. Es, por lo tanto, un proceso de inducción del conocimiento.\" --Wikipedia\n",
    "\n",
    "![alt text](https://www.explainxkcd.com/wiki/images/b/b2/twitter_bot.png \"Twitter Bot\")\n",
    "\n",
    "Machine Learning es encontrar de manera automatica patrones en un conjunto de datos para luego usar estos patrones en el futuro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seccion3\"></a>\n",
    "### Que es análisis de sentimiento\n",
    "_________________________________________________________________________________\n",
    "\n",
    "Es encontrar opiniones (Positivas/Negativas) en un texto. En nuestro caso de manera automatica usando ML. El análisis de sentimientos se aplica ampliamente a los comentarios y las redes sociales para una variedad de aplicaciones, que van desde el marketing hasta el servicio al cliente. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seccion4\"></a>\n",
    "### Que es clasificación\n",
    "_________________________________________________________________________________\n",
    "\n",
    "![alt text](http://ipython-books.github.io/images/ml.png \"Clasificacion y regresion\")\n",
    "Clasificación es por ejemplo definir si en la foto hay un gato o un perro  \n",
    "Regresión es obtener un estimado de las ventas en millones del proximo mes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seccion5\"></a>\n",
    "### Como funciona (visualmente) SVM\n",
    "_________________________________________________________________________________\n",
    "\n",
    "A diferencia de otros clasificadores, SVM buscan de forma explícita la mejor línea de separación. ¿Cómo? Los SVM buscan puntos más cercanos entre varias clases, que se llama \"los vectores de soporte\".\n",
    "\n",
    "Una vez que encuentra los puntos más cercanos, los SVM dibuja una línea que los conecta (ej. la línea 'w' en la figura). Los SVM luego declaran que la mejor línea de separación va a ser la línea que divide en dos - y es perpendicular a - la línea de conexión entre los puntos.\n",
    "![alt text](http://i.stack.imgur.com/ZA29t.png \"SVM\")\n",
    "\n",
    "#### Mas a profundidad (7 minutos)\n",
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=1NxnPkZM9bc\n",
    "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/1NxnPkZM9bc/0.jpg\" \n",
    "alt=\"How SVM (Support Vector Machine) algorithm works.\" width=\"560\" height=\"315\" border=\"10\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seccion6\"></a>\n",
    "### Como funciona (visualmente) Logistic Regression\n",
    "_________________________________________________________________________________\n",
    "\n",
    "A diferencia de los SVM, la Regresión logística se construye mediante la adopción de un punto a la vez y el ajuste de la línea divisoria en consecuencia. Tan pronto como todos los puntos esten separados, el algoritmo de regresión logística se detiene. Pero podría parar en cualquier lugar. La figura de abajo, muestra que hay un montón de diferentes líneas divisorias que separan los datos.\n",
    "![alt text](http://i.stack.imgur.com/kFSP3.png \"Regresion logistica\")\n",
    "\n",
    "#### Mas a profundidad (18 minutos)\n",
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=TycFjBx_-qE\n",
    "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/TycFjBx_-qE/0.jpg\" \n",
    "alt=\"Lesson49 Logistic Regression.\" width=\"560\" height=\"315\" border=\"10\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seccion7\"></a>\n",
    "### Librerias en python para generar modelos y manipular los datos\n",
    "_________________________________________________________________________________\n",
    "\n",
    "#### Pandas\n",
    " Pandas es una librería de python destinada al análisis de datos, que proporciona unas estructuras de datos flexibles y que permiten trabajar con ellos de forma muy eficiente. Pandas ofrece las siguientes estructuras de datos:\n",
    "\n",
    "* **Series:** Son arrays unidimensionales con indexación (arrays con índice o etiquetados), similar a los diccionarios. Pueden generarse a partir de diccionarios o de listas.\n",
    "* **DataFrame:** Son estructuras de datos similares a las tablas de bases de datos relacionales como SQL.\n",
    "* **Panel, Panel4D y PanelND:** Estas estructuras de datos permiten trabajar con más de dos dimensiones.\n",
    "\n",
    "Un excelente notebook con ejemplos de pandas: http://nbviewer.jupyter.org/github/rasbt/python_reference/blob/master/tutorials/things_in_pandas.ipynb\n",
    "\n",
    "\n",
    "#### Scikit - Learn\n",
    "\n",
    "Es una herramienta simple y eficiente para data mining y data analysis.\n",
    "Esta construida encima de las librerias NumPy, SciPy, y matplotlib.\n",
    "Es open source, con licensia BSD license (Comercialmente usable).\n",
    "\n",
    "Notebooks del creador de scikit-learn con muchos ejemplos: https://github.com/ogrisel/notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seccion8\"></a>\n",
    "### Como evaluar el trabajo\n",
    "_________________________________________________________________________________\n",
    "\n",
    "![alt text](https://chrisjmccormick.files.wordpress.com/2013/07/10_fold_cv.png \"Validacion cruzada\")\n",
    "Tomado de: https://chrisjmccormick.wordpress.com/\n",
    "\n",
    "Cross-validation: los datos de muestra (conocidos) se dividen en K subconjuntos. Uno de los subconjuntos se utiliza como datos de prueba y el resto (K-1) como datos de entrenamiento. El proceso de validación cruzada es repetido durante k iteraciones, con cada uno de los posibles subconjuntos de datos de prueba. Finalmente se realiza la media aritmética de los resultados de cada iteración para obtener un único resultado. Este método es muy preciso puesto que evaluamos a partir de K combinaciones de datos de entrenamiento y de prueba, pero aun así tiene una desventaja, y es que, a diferencia del método de retención, es lento desde el punto de vista computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seccion9\"></a>\n",
    "### Ensamblar modelos\n",
    "_________________________________________________________________________________\n",
    "\n",
    "El objetivo es unir las predicciones de distintos modelos de alguna manera.\n",
    "\n",
    "##### Ejemplos:\n",
    "\n",
    "![alt text](http://manish-m.com/wp-content/uploads/2012/11/Voting_Cropped.png \"Ensamble por votacion\")\n",
    "Tomado de: http://manish-m.com/\n",
    "\n",
    "**Se realiza una votacion (con o sin pesos) de los modelos para llegar a la prediccion final**\n",
    "\n",
    "![alt text](http://manish-m.com/wp-content/uploads/2012/11/StackingCropped.png \"Stacking / Meta ensambles\")\n",
    "Tomado de: http://manish-m.com/\n",
    "\n",
    "**Las predicciones de los modelos base se usan como input para modelos superiores **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seccion10\"></a>\n",
    "### Clasificar sentimiento en tweets de aerolineas\n",
    "_________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'core'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7ecb2872cc73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#Importar la libreria PANDAS para el manejo de datos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#importar la libreria NLTK para manejo de texto en python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/andrea/anaconda/lib/python3.5/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/andrea/anaconda/lib/python3.5/site-packages/pandas/core/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnotnull\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_eng_float_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m from pandas.core.index import (Index, CategoricalIndex, Int64Index,\n",
      "\u001b[0;32m/Users/andrea/anaconda/lib/python3.5/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                               DataError, SpecificationError)\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNDFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m from pandas.core.index import (Index, MultiIndex, CategoricalIndex,\n",
      "\u001b[0;32m/Users/andrea/anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m                                    \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                                    create_block_manager_from_blocks)\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseries\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpressions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/andrea/anaconda/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalAccessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstrings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m from pandas.tseries.common import (maybe_to_datetimelike,\n\u001b[1;32m     37\u001b[0m                                    CombinedDatetimelikeProperties)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'core'"
     ]
    }
   ],
   "source": [
    "# encoding=utf8\n",
    "\n",
    "# Creado por: Gabriel Farah\n",
    "# Email: g.farah38@uniandes.edu.co\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "#Forzar el encoding del archivo a UTF-8, hay tweets que no lo estan\n",
    "#import sys  \n",
    "#reload(sys)  \n",
    "#sys.setdefaultencoding('utf8')\n",
    "\n",
    "#Importar la libreria PANDAS para el manejo de datos\n",
    "import pandas as pd\n",
    "\n",
    "#importar la libreria NLTK para manejo de texto en python\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#Importar lo las funciones, clases y algoritmos que utilizaremos de Scikit-Learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn import decomposition, pipeline, metrics, grid_search\n",
    "\n",
    "#Importar la funcion de matriz no densa de scipy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "#Importar el modulo de expresiones regulares y de strings\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Crear una instancia de la clase porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#Usando pandas cargar la informacion de un archivo CSV\n",
    "pd.options.mode.chained_assignment = None\n",
    "df = pd.read_csv('Tweets.csv')\n",
    "\n",
    "#Seleccionar las columnas del archivo que vamos a usar y descartar el resto\n",
    "columns_to_keep = [u'airline_sentiment',u'retweet_count', u'airline', u'text']\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Veamos que aerolineas hay\n",
    "pd.Series(df[\"airline\"]).value_counts().plot(kind = \"bar\",figsize=(8,6),rot = 0, title = \"Aerolineas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(df[\"airline_sentiment\"]).value_counts().plot(kind = \"bar\" ,figsize=(8,6),color=['r', 'g', 'b'],rot=0, title = \"Categorias de sentimiento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(index = df[\"airline\"],columns = df[\"airline_sentiment\"]).plot(kind='bar',figsize=(10, 6),alpha=0.5,rot=0,stacked=True,title=\"Sentimiento por aerolinea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transformar la columna 'airline_sentiment' a formato numerico y borrar la columna antigua\n",
    "df.loc[:,'sentiment'] = df.airline_sentiment.map({'negative':-1,'neutral':0,'positive':1})\n",
    "df = df.drop(['airline_sentiment'], axis=1)\n",
    "\n",
    "#Limitar los datos de tweets a 1 retweet\n",
    "df = df[df['retweet_count'] <= 2]\n",
    "\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_tweet(s):\n",
    "\t'''\n",
    "\t:s : string; a tweet\n",
    "\n",
    "\t:return : list; words that don't contain url, @somebody, and in utf-8 and lower case\n",
    "\t'''\n",
    "\ts = re.sub(clean_tweet.pattern_airline, '', s, 1).decode('utf-8')\n",
    "\t#remove_punctuation_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "\t#print string.punctuation#remove_punctuation_map\t\n",
    "\t#s = s.translate(str(string.punctuation))#remove_punctuation_map)\n",
    "\texclude = set(string.punctuation)\n",
    "\ts = ''.join(ch for ch in s if ch not in exclude)\n",
    "\tsents = sent_tokenize(s)\n",
    "\n",
    "\twords = [word_tokenize(s) for s in sents]\n",
    "\twords = [e for sent in words for e in sent] \n",
    "\twords = [clean_tweet.stemmer.stem(e.lower()) for e in words]\n",
    "\treturn words#' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stop_words(s, n):\n",
    "\t'''\n",
    "\t:s : pd.Series; each element as a list of words from tokenization\n",
    "\t:n : int; n most frequent words are judged as stop words \n",
    "\n",
    "\t:return : list; a list of stop words\n",
    "\t'''\n",
    "\tfrom collections import Counter\n",
    "\tl = get_corpus(s)\n",
    "\tl = [x for x in Counter(l).most_common(n)]\n",
    "\treturn l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_corpus(s):\n",
    "\t'''\n",
    "\t:s : pd.Series; each element as a list of words from tokenization\n",
    "\n",
    "\t:return : list; corpus from s\n",
    "\t'''\n",
    "\tl = []\n",
    "\ts.map(lambda x: l.extend(x))\n",
    "\treturn l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Por cada Tweet limpiarlo reemplazando el valor original de la columna texto\n",
    "clean_tweet.stemmer = PorterStemmer()\n",
    "clean_tweet.pattern_airline = re.compile(r'@\\w+')\n",
    "df.loc[:,'text'] = df.loc[:,'text'].map(clean_tweet)\n",
    "\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Obtenemos las palabras mas comunes directamente del corpus \n",
    "freqwords = get_stop_words(df['text'],n=100)\n",
    "freq = [s[1] for s in freqwords]\n",
    "stopwords = [w[0] for w in freqwords[:18]]\n",
    "\n",
    "display(freqwords[:18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transformar   la lista de palabras a string para utilizar el Vectorizer\n",
    "df['text'] = df['text'].apply(lambda x: \"%s\" % ' '.join(x))\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Hacemos encoding a la clumna 'airline' a un formato numerico\n",
    "le = LabelEncoder()\n",
    "df['airline'] = le.fit_transform(df['airline'])\n",
    "\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Separamos nuestro target y el texto del resto del dataframe y los eliminamos de este\n",
    "tweets = df.text.values\n",
    "y = df.sentiment.values\n",
    "df = df.drop(['text','sentiment'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Las dimensiones de nuestro dataframe son: \",df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transformamos el texto usando un TFIDF vectorizer (hay mucho por optimizar aca!)\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words = stopwords)\n",
    "\n",
    "# Fit TFIDF\n",
    "tfv.fit(tweets)\n",
    "\n",
    "#Unimos los features de la matriz TFIDF a nuestro 'retweet_count' y 'airline'\n",
    "X = hstack([df.as_matrix().astype(float), tfv.transform(tweets)])\n",
    "\n",
    "print \"Las dimensiones de nuestro dataframe son ahora: \", X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Guardamos nuestro vectorizer\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(tfv, 'vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print \"Las dimensiones de nuestros datos de entrenamiento son: \", train_x.shape, train_y.shape\n",
    "print \"Las dimensiones de nuestros datos de prueba son: \", test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inicializamos el SVD porque la dimensionalidad es muy grande! 14k!\n",
    "svd = TruncatedSVD()\n",
    "\n",
    "# Inicializamos el standard scaler para estandarizar nuestros datos\n",
    "scl = StandardScaler()\n",
    "\n",
    "# Aca usamos un Support Vector Machine\n",
    "svm_model = SVC()\n",
    "\n",
    "# Creamos un Pipeline de procesamiento. Los datos pasan secuencialmente uno por uno: \n",
    "# Normalizar ->  Reducir dimensionalidad -> Generar el modelo \n",
    "clf = pipeline.Pipeline([('svd', svd),\n",
    "\t\t\t\t\t\t ('scl', scl),\n",
    "                \t     ('svm', svm_model)])\n",
    "\n",
    "# Para encontrar los mejores hyper-parametros, creamos un diccionario con algunos valores posibles para cada elemento \n",
    "# del Pipeline\n",
    "param_grid = {'svd__n_components' : [200, 400],\n",
    "              'svm__C': [10, 15],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inicializamos el Grid Search Model\n",
    "model = grid_search.GridSearchCV(estimator = clf, param_grid=param_grid, scoring='f1_weighted',\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "                                 \n",
    "# Aca encontramos los mejores hyper-parametros con el mejor puntaje\n",
    "model.fit(train_x, train_y)\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "\tprint(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Realizamos predicciones sobre los datos de prueba\n",
    "predictions = model.best_estimator_.predict(test_x)\n",
    "\n",
    "# Y validamos las predicciones de las pruebas vs los valores reales\n",
    "print(metrics.classification_report(test_y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Guardamos el modelo.\n",
    "joblib.dump(clf, 'modelo.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "\n",
    "# Cargamos otros modelos de clasificacion distintos para generar el ensamble\n",
    "logistic = LogisticRegression()\n",
    "sgd = SGDClassifier()\n",
    "\n",
    "# Empequetamos el mejor SVC con los otros dos modelos en un meta modelo de voto por mayoria\n",
    "ensamble = VotingClassifier(estimators=[('best_svm', model.best_estimator_), ('logistic', logistic), \n",
    "                                        ('sgd', sgd)], voting='hard')\n",
    "\n",
    "\n",
    "# Para encontrar los mejores hyper-parametros, creamos un diccionario con algunos valores posibles para cada elemento \n",
    "# del Meta-Modelo\n",
    "param_grid = {'sgd__alpha' : [0.0001, 0.001],\n",
    "              'logistic__C': [2.5, 5.0],}\n",
    "\n",
    "# Inicializamos el Grid Search Model\n",
    "model = grid_search.GridSearchCV(estimator = ensamble, param_grid=param_grid, scoring='f1_weighted',\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "                                 \n",
    "# Aca encontramos los mejores hyper-parametros con el mejor puntaje\n",
    "model.fit(train_x, train_y)\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "\tprint(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.best_estimator_.fit(train_x,train_y)\n",
    "\n",
    "# Generamos las predicciones del modelo de ensamble\n",
    "predictions = model.best_estimator_.predict(test_x)\n",
    "\n",
    "# validamos las predicciones con la realidad\n",
    "print(metrics.classification_report(test_y, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
